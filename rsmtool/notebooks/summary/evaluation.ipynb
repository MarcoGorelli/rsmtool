{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall association statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tables in this section show the standard association metrics between human scores and different types of machine scores. These results are computed on the evaluation set. The scores for each model have been truncated to [min-0.4998, max+.4998].When indicated, scaled scores are computed by re-scaling the predicted scores using mean and standard deviation of human scores as observed on the training data and mean and standard deviation of machine scores as predicted for the training set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_evals(model_list):\n",
    "    evals = []\n",
    "    for (model_id, config, csvdir) in model_list:\n",
    "        eval_short_file = os.path.join(csvdir, '{}_eval_short.csv'.format(model_id))\n",
    "        if os.path.exists(eval_short_file):\n",
    "            df_eval = pd.read_csv(eval_short_file, index_col=0)\n",
    "            df_eval.index = [model_id]\n",
    "            \n",
    "            # figure out whether the score was scaled\n",
    "            df_eval['system score type'] = 'scale' if config['use_scaled_predictions'] == True or config['scale_with'] is not None else 'raw'        \n",
    "            #rename the columns to remove reference to scale/raw scores\n",
    "            new_column_names = [col.split('.')[0] if not 'round' in col \n",
    "                                else '{} (rounded)'.format(col.split('.')[0])\n",
    "                                for col in df_eval.columns ]\n",
    "            df_eval.columns = new_column_names\n",
    "            # Get human-human scores from the other file\n",
    "            consistency_csv_file = os.path.join(csvdir, '{}_consistency.csv'.format(model_id))\n",
    "            if os.path.exists(consistency_csv_file):\n",
    "                consistency_df = pd.read_csv(consistency_csv_file, index_col=0)\n",
    "                df_eval['h-h-wtkappa'] = consistency_df.iloc[0]['wtkappa']\n",
    "                df_eval['h-h-corr'] = consistency_df.iloc[0]['corr']\n",
    "                df_eval['h-h-exact_agr'] = consistency_df.iloc[0]['exact_agr']\n",
    "                df_eval['h-h-adj_agr'] = consistency_df.iloc[0]['adj_agr']\n",
    "                df_eval['h-h-kappa'] = consistency_df.iloc[0]['kappa']\n",
    "            evals.append(df_eval) \n",
    "    if len(evals) > 0:\n",
    "        df_evals = pd.concat(evals)\n",
    "    else:\n",
    "        df_evals = pd.DataFrame()\n",
    "    return(df_evals)\n",
    "\n",
    "df_eval = read_evals(model_list)\n",
    "if not df_eval.empty:\n",
    "    df_eval.to_csv(join(output_dir, '{}_eval_short.csv'.format(summary_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename columns and add additional comparison columns\n",
    "\n",
    "# Ensure that all numeric columns are actually numbers\n",
    "# (They probably are, but just in case)\n",
    "df_eval.apply(pd.to_numeric, errors='ignore')\n",
    "# Rename columns to clarify human-machine vs human-human\n",
    "df_eval.rename(columns = {'corr':'h-m-corr', 'wtkappa (rounded)':'h-m-wtkappa (rounded)',\n",
    "                          'exact_agr (rounded)': 'h-m-exact_agr (rounded)',\n",
    "                          'adj_agr (rounded)': 'h-m-adj_agr (rounded)', \n",
    "                          'kappa (rounded)': 'h-m-kappa (rounded)'}, inplace = True)\n",
    "# Add additional columns showing difference between scores\n",
    "df_eval['h-h-corr h-m-corr diff'] = df_eval['h-h-corr'] - df_eval['h-m-corr']\n",
    "df_eval['h-h-wtkappa h-m-wtkappa diff'] = df_eval['h-h-wtkappa'] - df_eval['h-m-wtkappa (rounded)']\n",
    "df_eval['h-h-exact_agr h-m-exact_agr diff'] = df_eval['h-h-exact_agr'] - df_eval['h-m-exact_agr (rounded)']\n",
    "df_eval['h-h-adj_agr h-m-adj_agr diff'] = df_eval['h-h-adj_agr'] - df_eval['h-m-adj_agr (rounded)']\n",
    "df_eval['h-h-kappa h-m-kappa diff'] = df_eval['h-h-kappa'] - df_eval['h-m-kappa (rounded)']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive holistic score statistics\n",
    "\n",
    "The table shows distributional properties of human and system scores. SMD values lower then -0.15 or higher than 0.15 are <span class=\"highlight_color\">highlighted</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.width=10\n",
    "formatter = partial(color_highlighter, low=-0.15, high=0.15)\n",
    "if not df_eval.empty:\n",
    "     display(HTML(df_eval[['N', 'system score type', 'h_mean', 'h_sd',  'sys_mean', 'sys_sd',  'SMD']].to_html(index=True, classes = ['sortable'],\n",
    "                                                                                                               escape=False,\n",
    "                                                                                                               formatters={'SMD': formatter},\n",
    "                                                                                                               float_format=int_or_float_format_func)))\n",
    "else:\n",
    "     display(Markdown(\"No information available for any of the models\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Association statistics\n",
    "\n",
    "The table shows the standard association metrics between human scores and machine scores. Note that some evaluations are based on rounded (`Trim-round`) scores computed by first truncating and then rounding the predicted score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not df_eval.empty:\n",
    "     display(HTML(df_eval[['N',\n",
    "                           'system score type', 'h-h-corr',\n",
    "                           'h-m-corr', 'h-h-corr h-m-corr diff', 'R2', 'RMSE', 'h-h-wtkappa',\n",
    "                           'h-m-wtkappa (rounded)', 'h-h-wtkappa h-m-wtkappa diff', \n",
    "                           'h-h-kappa', 'h-m-kappa (rounded)', 'h-h-kappa h-m-kappa diff',\n",
    "                           'h-h-exact_agr', 'h-m-exact_agr (rounded)', 'h-h-exact_agr h-m-exact_agr diff',\n",
    "                           'h-h-adj_agr', 'h-m-adj_agr (rounded)', \n",
    "                           'h-h-adj_agr h-m-adj_agr diff']].to_html(index=True, classes = ['sortable'],\n",
    "                                                                                escape=False,\n",
    "                                                                                float_format = int_or_float_format_func)))\n",
    "else:\n",
    "     display(Markdown(\"No information available for any of the models\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
